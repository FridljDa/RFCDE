---
title: "RFCDE"
author: "Taylor Pospisil"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{RFCDE}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Random forests is a common non-parametric regression technique which
performs well for mixed-type data and irrelevant covariates, while
being robust to monotonic variable transformations. RFCDE fits random
forest models optimized for nonparametric conditional density
estimation.

## Example

```{r}
#library(RFCDE)
devtools::load_all() 
set.seed(42)

generate_data <- function(n) {
  x_relevant <- matrix(runif(n * 10), n, 10)
  x_irrelevant <- matrix(runif(n * 10), n, 10)
  z <- rnorm(n, rowSums(x_relevant), 1)
  return(list(x = cbind(x_relevant, x_irrelevant), z = z))
}

n_train <- 100
n_test <- 4

train_data <- generate_data(n_train)
x_train <- train_data$x
z_train <- train_data$z
```

### Training

Trees are recursively partitioned to minimize the CDE loss

$$ \int \int (\hat{f}(x \mid z) - f(x \mid z))^{2} dz dP(x) $$

This is efficiently calculated using an orthogonal series
representation of the conditional densities. The resolution of this
representation is controlled by `n_basis`.

```{r}
n_trees <- 100
mtry <- 10
node_size <- 20
n_basis <- 15

forest <- RFCDE(x_train, z_train, n_trees = n_trees, mtry = mtry,
                node_size = node_size, n_basis = n_basis, basis_system = "cosine")
```
```{r}
 oc0<- stringr::str_count("1100010010111011001100101011110010101001111101110110101110010101101111101110000110001110100101101000110111001010110011011111010010011011110111110010001010011111000000111111010011111111010100111010001110101101101101101101101000010111110010111111111000101101100101011101001100111011010
", "0")
 oc1<- stringr::str_count("1100010010111011001100101011110010101001111101110110101110010101101111101110000110001110100101101000110111001010110011011111010010011011110111110010001010011111000000111111010011111111010100111010001110101101101101101101101000010111110010111111111000101101100101011101001100111011010", "1")
 oc1/(oc0+oc1)
```
```{r}
stdout <- vector('character')
con    <- textConnection('stdout', 'wr', local = TRUE)
sink(con)
1:10
sink()
close(con)
stdout
```


### Prediction

We use the forest structure to determine weights for a weighted kernel
density estimate. The `predict` function evaluates this density on the
provided grid.

```{r}
bandwidth <- 0.2
n_grid <- 100
z_grid <- seq(0, 10, length.out = n_grid)
x_test <- generate_data(1)$x

density <- predict(forest, x_test, "CDE", z_grid, bandwidth = bandwidth)
```
```{r}
#groups <- apply(x_test, 1, function(row){
  forest$rcpp$traverse_forest(x_test)
#})
#groups <-t(groups)
#groups_5 <- groups[,1]


#image(seq(0, 1, length.out = n_grid), xlab = "X1",
#      seq(0, 1, length.out = n_grid), ylab = "X2",
#      z = matrix(groups_1, n_grid, n_grid))
```

```{r}
plot(z_grid, density, type = "l", col = "green")
lines(z_grid, dnorm(z_grid, sum(x_test[1:10]), 1))
```